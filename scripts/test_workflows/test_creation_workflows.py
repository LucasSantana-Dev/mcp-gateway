#!/usr/bin/env python3
"""
Test Creation Workflows for MCP Gateway Project

Implements structured workflows for creating high-quality tests
following the principles defined in the test creation plan.

Quality Over Quantity: Always create tests that provide real confidence.
"""

import os
import sys
import json
import argparse
import ast
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from abc import ABC, abstractmethod


@dataclass
class TestScenario:
    """Represents a test scenario with its metadata."""
    name: str
    description: str
    test_type: str  # unit, integration, e2e
    business_value: str
    edge_cases: List[str]
    dependencies: List[str]
    mock_requirements: List[str]


@dataclass
class TestQualityMetrics:
    """Metrics for evaluating test quality."""
    business_logic_coverage: float
    error_scenario_coverage: float
    mock_isolation_score: float
    realistic_data_score: float
    overall_quality_score: float


class TestWorkflow(ABC):
    """Abstract base class for test creation workflows."""
    
    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.tool_router_path = project_root / "tool_router"
        
    @abstractmethod
    def analyze_module(self, module_path: Path) -> Dict[str, Any]:
        """Analyze module functionality and identify test requirements."""
        pass
    
    @abstractmethod
    def design_scenarios(self, analysis: Dict[str, Any]) -> List[TestScenario]:
        """Design test scenarios based on module analysis."""
        pass
    
    @abstractmethod
    def implement_tests(self, scenarios: List[TestScenario], output_path: Path) -> str:
        """Generate test code for the scenarios."""
        pass
    
    @abstractmethod
    def validate_quality(self, test_code: str) -> TestQualityMetrics:
        """Validate the quality of generated tests."""
        pass


class UnitTestWorkflow(TestWorkflow):
    """Workflow for creating unit tests with business logic focus."""
    
    def analyze_module(self, module_path: Path) -> Dict[str, Any]:
        """Analyze module for unit testing requirements."""
        print(f"üîç Analyzing module: {module_path}")
        
        # Read module content
        try:
            with open(module_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except Exception as e:
            return {"error": f"Could not read module: {e}"}
        
        analysis = {
            "module_path": str(module_path),
            "functions": [],
            "classes": [],
            "business_logic": [],
            "error_conditions": [],
            "external_dependencies": []
        }
        
        # Parse Python AST for analysis
        try:
            import ast
            tree = ast.parse(content)
            
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    func_info = {
                        "name": node.name,
                        "line": node.lineno,
                        "args": [arg.arg for arg in node.args.args],
                        "has_business_logic": self._has_business_logic(node),
                        "error_scenarios": self._identify_error_scenarios(node)
                    }
                    analysis["functions"].append(func_info)
                    
                elif isinstance(node, ast.ClassDef):
                    class_info = {
                        "name": node.name,
                        "line": node.lineno,
                        "methods": []
                    }
                    for item in node.body:
                        if isinstance(item, ast.FunctionDef):
                            class_info["methods"].append(item.name)
                    analysis["classes"].append(class_info)
                    
        except SyntaxError as e:
            analysis["syntax_error"] = str(e)
        
        return analysis
    
    def _has_business_logic(self, node: ast.AST) -> bool:
        """Check if function contains business logic worth testing."""
        business_indicators = [
            'if', 'for', 'while', 'try', 'except', 'with',
            'return', 'yield', 'assert', 'raise'
        ]
        
        # Simple heuristic: check for business logic keywords
        for child in ast.walk(node):
            if isinstance(child, (ast.If, ast.For, ast.While, ast.Try, 
                               ast.With, ast.Return, ast.Raise)):
                return True
        return False
    
    def _identify_error_scenarios(self, node: ast.AST) -> List[str]:
        """Identify potential error scenarios in function."""
        scenarios = []
        
        for child in ast.walk(node):
            if isinstance(child, ast.ExceptHandler):
                scenarios.append(f"Exception: {child.type}")
            elif isinstance(child, ast.Call):
                # Look for external calls that might fail
                if isinstance(child.func, ast.Name):
                    scenarios.append(f"External call: {child.func.id}")
        
        return scenarios
    
    def design_scenarios(self, analysis: Dict[str, Any]) -> List[TestScenario]:
        """Design unit test scenarios focusing on business logic."""
        scenarios = []
        
        if "error" in analysis:
            return scenarios
        
        for func in analysis["functions"]:
            if func["has_business_logic"]:
                # Happy path scenario
                scenarios.append(TestScenario(
                    name=f"test_{func['name']}_success",
                    description=f"Test {func['name']} with valid inputs",
                    test_type="unit",
                    business_value=f"Verifies core functionality of {func['name']}",
                    edge_cases=[],
                    dependencies=[],
                    mock_requirements=[]
                ))
                
                # Error scenarios
                for error in func["error_scenarios"]:
                    scenarios.append(TestScenario(
                        name=f"test_{func['name']}_error_{error.lower().replace(' ', '_')}",
                        description=f"Test {func['name']} error handling for {error}",
                        test_type="unit",
                        business_value=f"Ensures robustness of {func['name']}",
                        edge_cases=[error],
                        dependencies=[],
                        mock_requirements=["patch_external_calls"]
                    ))
        
        return scenarios
    
    def implement_tests(self, scenarios: List[TestScenario], output_path: Path) -> str:
        """Generate unit test code with proper mocking and assertions."""
        test_code = []
        
        # Add imports
        test_code.append("""
import pytest
from unittest.mock import patch, MagicMock, AsyncMock
import sys
import os

# Add the project root to the path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', '..'))
""")
        
        # Group scenarios by function
        functions = {}
        for scenario in scenarios:
            func_name = scenario.name.replace('test_', '').split('_')[0]
            if func_name not in functions:
                functions[func_name] = []
            functions[func_name].append(scenario)
        
        # Generate tests for each function
        for func_name, func_scenarios in functions.items():
            test_code.append(f"\n\nclass Test{func_name.title()}:")
            test_code.append('    """Test cases for {func_name} functionality."""\n')
            
            for scenario in func_scenarios:
                test_code.append(self._generate_test_method(scenario))
        
        return '\n'.join(test_code)
    
    def _generate_test_method(self, scenario: TestScenario) -> str:
        """Generate a single test method."""
        method_lines = []
        
        # Method signature
        method_lines.append(f"    def {scenario.name}(self) -> None:")
        method_lines.append(f'        """{scenario.description}"""')
        
        # Setup mocks if needed
        if scenario.mock_requirements:
            for mock in scenario.mock_requirements:
                if mock == "patch_external_calls":
                    method_lines.append("        # Mock external dependencies")
                    method_lines.append("        with patch('module.external_function') as mock_external:")
                    method_lines.append("            mock_external.return_value = 'mocked_result'")
        
        # Test implementation
        if "error" in scenario.name.lower():
            method_lines.append("        # Test error scenario")
            method_lines.append("        with pytest.raises(Exception):")
            method_lines.append("            # Call function that should raise exception")
            method_lines.append("            pass")
        else:
            method_lines.append("        # Test success scenario")
            method_lines.append("        result = function_under_test()")
            method_lines.append("        assert result is not None")
            method_lines.append("        assert isinstance(result, expected_type)")
        
        method_lines.append("")
        
        return '\n'.join(method_lines)
    
    def validate_quality(self, test_code: str) -> TestQualityMetrics:
        """Validate unit test quality against defined standards."""
        metrics = TestQualityMetrics(
            business_logic_coverage=0.0,
            error_scenario_coverage=0.0,
            mock_isolation_score=0.0,
            realistic_data_score=0.0,
            overall_quality_score=0.0
        )
        
        # Count business logic tests
        business_tests = test_code.count('def test_') - test_code.count('error')
        total_tests = test_code.count('def test_')
        if total_tests > 0:
            metrics.business_logic_coverage = (business_tests / total_tests) * 100
        
        # Count error scenario tests
        error_tests = test_code.count('error')
        if total_tests > 0:
            metrics.error_scenario_coverage = (error_tests / total_tests) * 100
        
        # Check for proper mocking
        mock_score = 0
        if 'patch(' in test_code:
            mock_score += 50
        if 'MagicMock' in test_code or 'AsyncMock' in test_code:
            mock_score += 50
        metrics.mock_isolation_score = mock_score
        
        # Check for realistic test data
        data_score = 0
        if 'realistic' in test_code.lower() or 'fixture' in test_code.lower():
            data_score += 50
        if 'assert' in test_code:
            data_score += 50
        metrics.realistic_data_score = data_score
        
        # Calculate overall score
        metrics.overall_quality_score = (
            metrics.business_logic_coverage * 0.3 +
            metrics.error_scenario_coverage * 0.3 +
            metrics.mock_isolation_score * 0.2 +
            metrics.realistic_data_score * 0.2
        )
        
        return metrics


class IntegrationTestWorkflow(TestWorkflow):
    """Workflow for creating integration tests."""
    
    def analyze_module(self, module_path: Path) -> Dict[str, Any]:
        """Analyze module for integration testing requirements."""
        print(f"üîó Analyzing module for integration testing: {module_path}")
        
        # For integration tests, focus on component interactions
        analysis = {
            "module_path": str(module_path),
            "integration_points": [],
            "data_flows": [],
            "external_systems": [],
            "configuration_dependencies": []
        }
        
        # Read module and look for integration patterns
        try:
            with open(module_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Look for imports (integration points)
            import_patterns = ['import ', 'from ']
            for line in content.split('\n'):
                for pattern in import_patterns:
                    if pattern in line and not line.strip().startswith('#'):
                        analysis["integration_points"].append(line.strip())
        
        except Exception as e:
            analysis["error"] = str(e)
        
        return analysis
    
    def design_scenarios(self, analysis: Dict[str, Any]) -> List[TestScenario]:
        """Design integration test scenarios."""
        scenarios = []
        
        if "error" in analysis:
            return scenarios
        
        # Create scenarios for each integration point
        for i, integration in enumerate(analysis["integration_points"]):
            scenarios.append(TestScenario(
                name=f"test_integration_{i+1}",
                description=f"Test integration with {integration}",
                test_type="integration",
                business_value=f"Ensures proper integration with {integration}",
                edge_cases=["network_failure", "invalid_response"],
                dependencies=[integration],
                mock_requirements=["external_system_mocks"]
            ))
        
        return scenarios
    
    def implement_tests(self, scenarios: List[TestScenario], output_path: Path) -> str:
        """Generate integration test code."""
        test_code = [
            """
import pytest
import asyncio
from unittest.mock import patch, AsyncMock
import tempfile
import os

# Integration test configuration
@pytest.fixture
def test_environment():
    \"\"\"Setup test environment for integration testing.\"\"\"
    with tempfile.TemporaryDirectory() as temp_dir:
        yield {
            "temp_dir": temp_dir,
            "config": {
                "database_url": "sqlite:///:memory:",
                "redis_url": "redis://localhost:6379/1"
            }
        }

@pytest.fixture
async def async_client():
    \"\"\"Setup async client for integration tests.\"\"\"
    # Setup async client here
    pass
"""
        ]
        
        for scenario in scenarios:
            test_code.append(f"""
    @pytest.mark.asyncio
    async def {scenario.name}(self, test_environment):
        \"\"\"{scenario.description}\"\"\"
        # Setup integration test
        async with AsyncClient() as client:
            # Test integration scenario
            response = await client.get("/test-endpoint")
            assert response.status_code == 200
            
            # Verify integration behavior
            data = response.json()
            assert "result" in data
""")
        
        return '\n'.join(test_code)
    
    def validate_quality(self, test_code: str) -> TestQualityMetrics:
        """Validate integration test quality."""
        metrics = TestQualityMetrics(
            business_logic_coverage=50.0,  # Integration tests focus on interactions
            error_scenario_coverage=30.0,
            mock_isolation_score=80.0,  # Should mock external systems
            realistic_data_score=70.0,
            overall_quality_score=65.0
        )
        
        # Adjust scores based on actual content
        if 'async def test' in test_code:
            metrics.business_logic_coverage += 20
        if 'AsyncMock' in test_code:
            metrics.mock_isolation_score += 10
        if 'tempfile' in test_code:
            metrics.realistic_data_score += 20
        
        metrics.overall_quality_score = (
            metrics.business_logic_coverage * 0.25 +
            metrics.error_scenario_coverage * 0.25 +
            metrics.mock_isolation_score * 0.35 +
            metrics.realistic_data_score * 0.15
        )
        
        return metrics


class EndToEndTestWorkflow(TestWorkflow):
    """Workflow for creating end-to-end tests."""
    
    def analyze_module(self, module_path: Path) -> Dict[str, Any]:
        """Analyze module for E2E testing requirements."""
        print(f"üéØ Analyzing module for E2E testing: {module_path}")
        
        analysis = {
            "module_path": str(module_path),
            "user_workflows": [],
            "system_interactions": [],
            "performance_requirements": [],
            "reliability_requirements": []
        }
        
        # For E2E, focus on complete user journeys
        return analysis
    
    def design_scenarios(self, analysis: Dict[str, Any]) -> List[TestScenario]:
        """Design end-to-end test scenarios."""
        scenarios = [
            TestScenario(
                name="test_complete_user_workflow",
                description="Test complete user workflow from start to finish",
                test_type="e2e",
                business_value="Ensures complete user journey works correctly",
                edge_cases=["slow_network", "server_restart", "resource_exhaustion"],
                dependencies=["full_system"],
                mock_requirements=["minimal_mocks"]  # E2E should use minimal mocks
            )
        ]
        
        return scenarios
    
    def implement_tests(self, scenarios: List[TestScenario], output_path: Path) -> str:
        """Generate end-to-end test code."""
        test_code = [
            """
import pytest
import asyncio
import time
from playwright.async_api import async_playwright

# E2E Test Configuration
@pytest.fixture
async def browser():
    \"\"\"Setup browser for E2E testing.\"\"\"
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context()
        page = await context.new_page()
        yield page
        await browser.close()

@pytest.fixture
async def test_server():
    \"\"\"Setup test server for E2E testing.\"\"\"
    # Start test server
    yield {"url": "http://localhost:8000"}
"""
        ]
        
        for scenario in scenarios:
            test_code.append(f"""
    @pytest.mark.e2e
    async def {scenario.name}(self, browser, test_server):
        \"\"\"{scenario.description}\"\"\"
        # Navigate to application
        await browser.goto(test_server["url"])
        
        # Complete user workflow
        await browser.fill("input[name='query']", "test search")
        await browser.click("button[type='submit']")
        
        # Wait for results
        await browser.wait_for_selector(".results")
        
        # Verify results
        results = await browser.query_selector_all(".result-item")
        assert len(results) > 0
        
        # Check performance
        load_time = await browser.evaluate("performance.now()")
        assert load_time < 5000  # Should load within 5 seconds
""")
        
        return '\n'.join(test_code)
    
    def validate_quality(self, test_code: str) -> TestQualityMetrics:
        """Validate E2E test quality."""
        metrics = TestQualityMetrics(
            business_logic_coverage=90.0,  # E2E focuses on complete workflows
            error_scenario_coverage=60.0,
            mock_isolation_score=20.0,  # Minimal mocking in E2E
            realistic_data_score=85.0,
            overall_quality_score=75.0
        )
        
        return metrics


class TestCreationOrchestrator:
    """Orchestrates the test creation workflows."""
    
    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.workflows = {
            "unit": UnitTestWorkflow(project_root),
            "integration": IntegrationTestWorkflow(project_root),
            "e2e": EndToEndTestWorkflow(project_root)
        }
    
    def create_tests(self, module_path: Path, test_type: str) -> Dict[str, Any]:
        """Create tests for a module using the specified workflow."""
        print(f"üöÄ Creating {test_type} tests for {module_path}")
        
        if test_type not in self.workflows:
            return {"error": f"Unknown test type: {test_type}"}
        
        workflow = self.workflows[test_type]
        
        # Execute workflow
        analysis = workflow.analyze_module(module_path)
        if "error" in analysis:
            return analysis
        
        scenarios = workflow.design_scenarios(analysis)
        if not scenarios:
            return {"warning": "No test scenarios generated"}
        
        # Generate output path
        relative_path = module_path.relative_to(self.project_root / "tool_router")
        test_dir = self.project_root / "tool_router" / "tests" / test_type
        test_dir.mkdir(parents=True, exist_ok=True)
        
        test_file_name = f"test_{relative_path.stem}.py"
        output_path = test_dir / test_file_name
        
        # Implement tests
        test_code = workflow.implement_tests(scenarios, output_path)
        
        # Validate quality
        metrics = workflow.validate_quality(test_code)
        
        # Write test file
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(test_code)
            
            return {
                "success": True,
                "test_file": str(output_path),
                "scenarios_created": len(scenarios),
                "quality_metrics": metrics,
                "test_code_preview": test_code[:500] + "..." if len(test_code) > 500 else test_code
            }
        except Exception as e:
            return {"error": f"Failed to write test file: {e}"}
    
    def batch_create_tests(self, module_pattern: str, test_type: str) -> List[Dict[str, Any]]:
        """Create tests for multiple modules matching a pattern."""
        results = []
        
        # Find matching modules
        tool_router_path = self.project_root / "tool_router"
        matching_modules = list(tool_router_path.rglob(module_pattern))
        
        if not matching_modules:
            return [{"error": f"No modules found matching pattern: {module_pattern}"}]
        
        for module_path in matching_modules:
            if module_path.is_file() and module_path.suffix == '.py':
                result = self.create_tests(module_path, test_type)
                result["module"] = str(module_path)
                results.append(result)
        
        return results


def main():
    """Main CLI interface."""
    parser = argparse.ArgumentParser(description="Create high-quality tests for MCP Gateway")
    parser.add_argument("command", choices=["create", "batch"], help="Command to run")
    parser.add_argument("module", help="Module path or pattern")
    parser.add_argument("--type", choices=["unit", "integration", "e2e"], 
                       default="unit", help="Type of tests to create")
    parser.add_argument("--project-root", default=".", help="Project root directory")
    
    args = parser.parse_args()
    
    project_root = Path(args.project_root).resolve()
    orchestrator = TestCreationOrchestrator(project_root)
    
    if args.command == "create":
        module_path = Path(args.module).resolve()
        if not module_path.exists():
            print(f"‚ùå Module not found: {module_path}")
            sys.exit(1)
        
        result = orchestrator.create_tests(module_path, args.type)
        
        if "error" in result:
            print(f"‚ùå Error: {result['error']}")
            sys.exit(1)
        elif "warning" in result:
            print(f"‚ö†Ô∏è  Warning: {result['warning']}")
        else:
            print(f"‚úÖ Successfully created {result['scenarios_created']} test scenarios")
            print(f"üìÅ Test file: {result['test_file']}")
            print(f"üìä Quality score: {result['quality_metrics'].overall_quality_score:.1f}%")
    
    elif args.command == "batch":
        results = orchestrator.batch_create_tests(args.module, args.type)
        
        for result in results:
            module_name = result.get("module", "unknown")
            if "error" in result:
                print(f"‚ùå {module_name}: {result['error']}")
            elif "warning" in result:
                print(f"‚ö†Ô∏è  {module_name}: {result['warning']}")
            else:
                print(f"‚úÖ {module_name}: {result['scenarios_created']} scenarios, "
                      f"quality: {result['quality_metrics'].overall_quality_score:.1f}%")


if __name__ == "__main__":
    main()